{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime benchmarks\n",
    "\n",
    "This notebook is designed to calculate benchmark runtimes on the ORIO website.\n",
    "\n",
    "Note that benchmarks are highly dependent on the number of processors available; the feature-list count-matrixes are parallelized using a task manager, and are also cached so the don't need to be recalculated in the future.\n",
    "\n",
    "This script is desiged for re-runs.\n",
    "\n",
    "## User inputs, modify environment:\n",
    "\n",
    "Set environment variables as needed before running:\n",
    "\n",
    "```bash\n",
    "export \"ORIO_BENCHMARK_EMAIL=foo@bar.com\"\n",
    "export \"ORIO_BENCHMARK_FEATURELIST=/path/to/hg19_fake.filtered.bed\"\n",
    "```\n",
    "\n",
    "To execute:\n",
    "\n",
    "```bash\n",
    "nohup python ./benchmarkReruns.py &> benchmarkReruns.out &\n",
    "```\n",
    "\n",
    "To get runtime results (dump django model):\n",
    "\n",
    "```bash\n",
    "python manage.py dumpdata analysis.Analysis -o ./analysis.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import os \n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import django\n",
    "from django.core.files import File \n",
    "\n",
    "django.setup()\n",
    "from analysis import models\n",
    "from myuser.models import User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup user inputs\n",
    "email = os.environ['ORIO_BENCHMARK_EMAIL']\n",
    "bigFeatureList = os.environ['ORIO_BENCHMARK_FEATURELIST']\n",
    "replicates = 3\n",
    "\n",
    "# tuple of (features, datasets) for re-runs\n",
    "pairs = [\n",
    "    (500, 10),\n",
    "    (100000, 10),\n",
    "    (100000, 750)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureNs = list(set([d[0] for d in pairs]))\n",
    "datasetNs = list(set([d[1] for d in pairs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear old benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user = User.objects.get(email=email)\n",
    "models.FeatureList.objects\\\n",
    "    .filter(owner=user, name__icontains='benchmarking:')\\\n",
    "    .delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature lists\n",
    "\n",
    "We take a list of over 130,000 features, and then randomly select a subset of features from this master set. Then, we create a list of FeatureLists, each with a different number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load big feature-list file\n",
    "with open(bigFeatureList, 'r') as f:\n",
    "    fls = f.readlines()\n",
    "\n",
    "fls = np.array(fls)\n",
    "print('{:,} lines'.format(fls.size))\n",
    "print('First line: %s ' % fls[0])\n",
    "print('Last line: %s' % fls[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFeatureList(features, size):\n",
    "    fl = features[np.random.choice(features.size, size, replace=False)]\n",
    "    f  = BytesIO()\n",
    "    bytestring = str.encode(''.join(fl.tolist()))\n",
    "    f.write(bytestring)\n",
    "    f.seek(0)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create feature-list objects in Django\n",
    "featureLists = {}\n",
    "for n in featureNs:\n",
    "    name = \"benchmarking: {} features\".format(n)\n",
    "    fl = models.FeatureList.objects.create(\n",
    "        owner=user,\n",
    "        name=name,\n",
    "        stranded=True,\n",
    "        genome_assembly_id=1,  # hg19\n",
    "    )    \n",
    "    fl.dataset.save(name+'.txt', File(getFeatureList(fls, n)))\n",
    "    fl.save()\n",
    "    fl.validate_and_save()\n",
    "    assert fl.validated is True\n",
    "    featureLists[n] = fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete existing feature-list count matrices; required becase\n",
    "# it will change the benchmarking behavavior because by \n",
    "# default the matrix can be re-used after initial exeuction.\n",
    "def deleteFlcm():\n",
    "    fls = list(featureLists.values())\n",
    "    models.FeatureListCountMatrix.objects\\\n",
    "        .filter(feature_list__in=fls)\\\n",
    "        .delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random dataset collections\n",
    "\n",
    "We randomly select a subset of encode datasets of varying sizes. To try to make the datasets a little more uniform for benchmarking, we first select the largest subset, and then iteratively select smaller subsets from each previous subset (that way we know that the smallest subset is guarenteed to a set of datasets which were previously run in a larger dataset.\n",
    "\n",
    "The end result is a list of datasets, going from smallest to largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get available datasets\n",
    "datasetLists = {}\n",
    "datasets = list(models.EncodeDataset.objects\\\n",
    "    .filter(genome_assembly_id=1)\\\n",
    "    .values_list('id', 'name'))\n",
    "\n",
    "# create subsets\n",
    "for n in reversed(sorted(datasetNs)):\n",
    "    subset_ids = np.random.choice(len(datasets), n, replace=False)\n",
    "    subset = [datasets[i] for i in subset_ids] \n",
    "    datasetLists[n] = [\n",
    "        dict(dataset=d[0], display_name=d[1]) \n",
    "        for d in subset\n",
    "    ]\n",
    "    datasets = subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create analyses\n",
    "\n",
    "We create and validate our analyses, where there will be a total of $i * j * k$, where $i$ is the number of feature lists, $j$ is the number of dataset lists, and $k$ is the number of replicates for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create analyses\n",
    "analyses = []\n",
    "for rep in range(replicates):\n",
    "    for n_fl, n_ds in pairs:    \n",
    "        a = models.Analysis.objects.create(\n",
    "            owner=user,\n",
    "            name=\"benchmarking: {} features, {} datasets\".format(n_fl, n_ds),\n",
    "            genome_assembly_id=1,  # hg19\n",
    "            feature_list=featureLists[n_fl],\n",
    "        )\n",
    "        a.save()        \n",
    "        objects = [\n",
    "            models.AnalysisDatasets(\n",
    "                analysis_id=a.id,\n",
    "                dataset_id=d['dataset'],\n",
    "                display_name=d['display_name'],\n",
    "            ) for d in datasetLists[n_ds]\n",
    "        ]\n",
    "        models.AnalysisDatasets.objects.bulk_create(objects)\n",
    "        a.validate_and_save()\n",
    "        assert a.validated is True\n",
    "        analyses.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Now, we iteratively execute each analysis. We don't start the next analysis until the previous has finished.\n",
    "\n",
    "Results are saved, and then transformed into a pandas DataFrame, and exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# execute\n",
    "results = []\n",
    "for i, analysis in enumerate(analyses):\n",
    "    print('Running {} of {}...'.format(i+1, len(analyses)))\n",
    "    deleteFlcm()\n",
    "    analysis.execute(silent=True)\n",
    "    while True:\n",
    "        time.sleep(3)\n",
    "        a = models.Analysis.objects.get(id=analysis.id)\n",
    "        if a.is_complete:\n",
    "            break\n",
    "            \n",
    "print('complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
