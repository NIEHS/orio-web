{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime benchmarks\n",
    "\n",
    "This notebook is designed to calculate benchmark runtimes on the ORIO website.\n",
    "\n",
    "Note that benchmarks are highly dependent on the number of processors available; the feature-list count-matrixes are parallelized using a task manager, and are also cached so the don't need to be recalculated in the future.\n",
    "\n",
    "\n",
    "## User inputs, modify environment:\n",
    "\n",
    "Set environment variables as needed before running:\n",
    "\n",
    "```bash\n",
    "export \"ORIO_BENCHMARK_EMAIL=foo@bar.com\"\n",
    "export \"ORIO_BENCHMARK_FEATURELIST=/path/to/hg19_fake.filtered.bed\"\n",
    "export \"ORIO_BENCHMARK_OUTPUT=/path/to/benchmark_output.txt\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import namedtuple\n",
    "from io import BytesIO\n",
    "import os \n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from django.core.files import File \n",
    "\n",
    "from analysis import models\n",
    "from myuser.models import User\n",
    "\n",
    "pd.options.display.mpl_style = 'default'  # ggplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup user inputs\n",
    "email = os.environ['ORIO_BENCHMARK_EMAIL']\n",
    "bigFeatureList = os.environ['ORIO_BENCHMARK_FEATURELIST']\n",
    "outputs = os.environ['ORIO_BENCHMARK_OUTPUT']\n",
    "replicates = 3\n",
    "featureNs = [10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000]\n",
    "datasetNs = [2, 5, 10, 50, 100, 500, 750]\n",
    "\n",
    "\n",
    "featureNs = [10, 50]\n",
    "datasetNs = [2, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear old benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user = User.objects.get(email=email)\n",
    "models.FeatureList.objects\\\n",
    "    .filter(owner=user, name__icontains='benchmarking:')\\\n",
    "    .delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature lists\n",
    "\n",
    "We take a list of over 130,000 features, and then randomly select a subset of features from this master set. Then, we create a list of FeatureLists, each with a different number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load big feature-list file\n",
    "with open(bigFeatureList, 'r') as f:\n",
    "    fls = f.readlines()\n",
    "\n",
    "fls = np.array(fls)\n",
    "print('{:,} lines'.format(fls.size))\n",
    "print('First line: %s ' % fls[0])\n",
    "print('Last line: %s' % fls[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFeatureList(features, size):\n",
    "    fl = features[np.random.choice(features.size, size, replace=False)]\n",
    "    f  = BytesIO()\n",
    "    bytestring = str.encode(''.join(fl.tolist()))\n",
    "    f.write(bytestring)\n",
    "    f.seek(0)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create feature-list objects in Django\n",
    "featureLists = []\n",
    "for n in featureNs:\n",
    "    name = \"benchmarking: {} features\".format(n)\n",
    "    fl = models.FeatureList.objects.create(\n",
    "        owner=user,\n",
    "        name=name,\n",
    "        stranded=True,\n",
    "        genome_assembly_id=1,  # hg19\n",
    "    )    \n",
    "    fl.dataset.save(name+'.txt', File(getFeatureList(fls, n)))\n",
    "    fl.save()\n",
    "    fl.validate_and_save()\n",
    "    featureLists.append((n, fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete existing feature-list count matrices; required becase\n",
    "# it will change the benchmarking behavavior because by \n",
    "# default the matrix can be re-used after initial exeuction.\n",
    "def deleteFlcm():\n",
    "    fls = [fl[1] for fl in featureLists]\n",
    "    models.FeatureListCountMatrix.objects\\\n",
    "        .filter(feature_list__in=fls)\\\n",
    "        .delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random dataset collections\n",
    "\n",
    "We randomly select a subset of encode datasets of varying sizes. To try to make the datasets a little more uniform for benchmarking, we first select the largest subset, and then iteratively select smaller subsets from each previous subset (that way we know that the smallest subset is guarenteed to a set of datasets which were previously run in a larger dataset.\n",
    "\n",
    "The end result is a list of datasets, going from smallest to largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get available datasets\n",
    "datasetLists = []\n",
    "datasets = list(models.EncodeDataset.objects\\\n",
    "    .filter(genome_assembly_id=1)\\\n",
    "    .values_list('id', 'name'))\n",
    "\n",
    "# create subsets\n",
    "for n in reversed(datasetNs):\n",
    "    subset_ids = np.random.choice(len(datasets), n, replace=False)\n",
    "    subset = [datasets[i] for i in subset_ids]\n",
    "    datasetLists.append([dict(dataset=d[0], display_name=d[1]) for d in subset])\n",
    "    datasets = subset\n",
    "    \n",
    "# switch order to go from smallest -> largest\n",
    "datasetLists = list(reversed(datasetLists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create analyses\n",
    "\n",
    "We create and validate our analyses, where there will be a total of $i * j * k$, where $i$ is the number of feature lists, $j$ is the number of dataset lists, and $k$ is the number of replicates for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create analyses\n",
    "analyses = []\n",
    "for fl in featureLists:\n",
    "    n_features = fl[0]\n",
    "    for ds in datasetLists:\n",
    "        for rep in range(replicates):\n",
    "            n_ds = len(ds)\n",
    "\n",
    "            a = models.Analysis.objects.create(\n",
    "                owner=user,\n",
    "                name=\"benchmarking: {} features, {} datasets\".format(n_features, n_ds),\n",
    "                genome_assembly_id=1,  # hg19\n",
    "                feature_list=fl[1],\n",
    "            )\n",
    "            a.save()        \n",
    "            objects = [\n",
    "                models.AnalysisDatasets(\n",
    "                    analysis_id=a.id,\n",
    "                    dataset_id=d['dataset'],\n",
    "                    display_name=d['display_name'],\n",
    "                ) for d in ds\n",
    "            ]\n",
    "            models.AnalysisDatasets.objects.bulk_create(objects)\n",
    "            a.validate_and_save()\n",
    "            analyses.append((a, n_features, n_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Now, we iteratively execute each analysis. We don't start the next analysis until the previous has finished.\n",
    "\n",
    "Results are saved, and then transformed into a pandas DataFrame, and exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# execute\n",
    "Analysis = namedtuple('Analysis', ('id', 'features', 'datasets', 'seconds'))\n",
    "results = []\n",
    "for i, analysis in enumerate(analyses):\n",
    "    print('Running {} of {}...'.format(i+1, len(analyses)))\n",
    "    deleteFlcm()\n",
    "    analysis[0].execute(silent=True)\n",
    "    while True:\n",
    "        time.sleep(3)\n",
    "        a = models.Analysis.objects.get(id=analysis[0].id)\n",
    "        if a.is_complete:\n",
    "            break\n",
    "    \n",
    "    duration = (a.end_time-a.start_time).total_seconds()\n",
    "    results.append(Analysis(a.id, analysis[1], analysis[2], duration))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save and export\n",
    "res = pd.DataFrame(results)\n",
    "res.head(10)\n",
    "res.to_csv(outputs, sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visuals\n",
    "\n",
    "Create a lineplot and regression for datasets vs runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset = res[res.features==10]\n",
    "f = subset.plot(kind='scatter', x='datasets', y='seconds', figsize=(12, 8))\n",
    "f.hold= True\n",
    "m = stats.linregress(x=subset.datasets.values, y=subset.seconds.values)\n",
    "xs = np.arange(0, subset.datasets.max()*1.2, 1)\n",
    "ys = m.intercept + m.slope * xs\n",
    "f.plot(xs, ys, 'r-')\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a scatterplot for features vs runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset = res[res.datasets==10]\n",
    "plt = subset.plot(kind='scatter', x='features', y='seconds', figsize=(12, 8))\n",
    "plt.hold= True\n",
    "m, residuals, rank, singular_values, rcond = np.polyfit(subset.features.values, subset.seconds.values, 2, full=True)\n",
    "m = list(reversed(m))\n",
    "xs = np.arange(0, subset.features.max()*1.2, 5)\n",
    "ys = m[0] + m[1] * xs + m[2] * np.power(xs, 2)\n",
    "plt.plot(xs, ys, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a contour plot, where x = features, y = datasets, and contour is runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap(cm.gist_earth)\n",
    "\n",
    "res2 = res.drop('id', axis=1)\n",
    "res2 = res2.groupby(['features', 'datasets']).mean()\n",
    "res2.head()\n",
    "\n",
    "X=res2.index.levels[0].values\n",
    "Y=res2.index.levels[1].values\n",
    "Z=res2.unstack(level=0).seconds.values\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ct = plt.contour(X, Y, Z)\n",
    "\n",
    "plt.clabel(ct, inline=1, fmt='%d', fontsize=12)\n",
    "plt.xlabel('N features')\n",
    "plt.ylabel('N datasets')\n",
    "plt.title('Total runtime for ORIO analysis')\n",
    "\n",
    "plt.colorbar(ct, orientation='horizontal', shrink=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
